{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['data_acuteinflammation',\n",
    "            'data_balancescale',\n",
    "            'data_breastcancerwisc',\n",
    "            'data_cardiotocography3clases',\n",
    "            'data_energyy1',\n",
    "            'data_energyy2',\n",
    "            'data_iris',\n",
    "            'data_mammographic',\n",
    "            'data_pendigits',\n",
    "            'data_seeds',\n",
    "            'data_tictactoe',\n",
    "            'data_vertebralcolumn2clases',\n",
    "            'data_vertebralcolumn3clases']\n",
    "\n",
    "target = [[ 200.,  240.,  400.,  500.,  300.,  300.,  200.,  300.,  600.,  280.,  340.,  280.,  240.],\n",
    "          [ 400.,  480.,  800., 1000.,  600.,  600.,  400.,  600., 1200.,  560.,  680.,  560.,  480.],\n",
    "          [ 600.,  720., 1200., 1500.,  900.,  900.,  600.,  900., 1800.,  840., 1020.,  840.,  720.],\n",
    "          [ 800.,  960., 1600., 2000., 1200., 1200.,  800., 1200., 2400., 1120., 1360., 1120.,  960.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 4, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets), len(target), len(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time for data_acuteinflammation: 9 days, 23 hours, 53 minutes and 32.50 seconds\n",
      "Total training time for data_balancescale: 3 days, 4 hours, 36 minutes and 25.80 seconds\n",
      "Total training time for data_breastcancerwisc: 4 days, 10 hours, 13 minutes and 1.00 seconds\n",
      "Total training time for data_cardiotocography3clases: 12 days, 8 hours, 24 minutes and 19.80 seconds\n",
      "Total training time for data_energyy1: 7 days, 17 hours, 25 minutes and 11.20 seconds\n",
      "Total training time for data_energyy2: 8 days, 17 hours, 51 minutes and 57.80 seconds\n",
      "Total training time for data_iris: 4 days, 11 hours, 40 minutes and 47.00 seconds\n",
      "Total training time for data_mammographic: 3 days, 21 hours, 46 minutes and 42.50 seconds\n",
      "Total training time for data_pendigits: 0 days, 0 hours, 0 minutes and 0.00 seconds\n",
      "Total training time for data_seeds: 3 days, 16 hours, 13 minutes and 36.80 seconds\n",
      "Total training time for data_tictactoe: 4 days, 10 hours, 51 minutes and 41.70 seconds\n",
      "Total training time for data_vertebralcolumn2clases: 3 days, 5 hours, 29 minutes and 4.80 seconds\n",
      "Total training time for data_vertebralcolumn3clases: 2 days, 23 hours, 58 minutes and 16.20 seconds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def calculate_total_training_time(log_file_path):\n",
    "    \"\"\"\n",
    "    Reads the log file, finds lines with 'Epoch: X' and 'Epoch time: Y',\n",
    "    and computes total training time *assuming* 'Epoch time: Y' is\n",
    "    the average time per epoch for the last (X - previous_epoch) epochs.\n",
    "    \"\"\"\n",
    "    total_time = 0.0\n",
    "    # This regex looks for a line containing something like:\n",
    "    # | Epoch:    50 | Train loss: ... | Epoch time: 0.2 |\n",
    "    pattern = re.compile(r'\\|\\s*Epoch:\\s*(\\d+)\\s*\\|.*Epoch time:\\s+([\\d.]+)')\n",
    "    \n",
    "    prev_epoch = None\n",
    "    with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                current_epoch = int(match.group(1))\n",
    "                epoch_time_avg = float(match.group(2))  # average time per epoch (assumed)\n",
    "                \n",
    "                if prev_epoch is None:\n",
    "                    # For the very first logged epoch, assume that the block covers all epochs from 0 up to current_epoch\n",
    "                    block_size = current_epoch + 1  # e.g., epoch=50 implies epochs 0..50 inclusive => 51 epochs\n",
    "                else:\n",
    "                    # Otherwise, the block covers (prev_epoch+1) .. current_epoch\n",
    "                    block_size = current_epoch - prev_epoch\n",
    "                \n",
    "                # Multiply average time by the block size\n",
    "                total_time += epoch_time_avg * block_size\n",
    "                prev_epoch = current_epoch\n",
    "\n",
    "    return total_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = datasets[0]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    # Replace 'path_to_your_log_file.log' with the actual path to your log file\n",
    "    baseline_total_array = []\n",
    "    for ds in datasets:\n",
    "        total = 0\n",
    "        for i in range(0,10):\n",
    "            for j in range(0,100,2):\n",
    "                log_path = f'/home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/PowerAwareBaselineTanh/log/{ds}_seed_{i}_Penalty_power_Factor_{j}.log'\n",
    "                total_training_time = 0\n",
    "                if os.path.exists(log_path):\n",
    "                    total_training_time = calculate_total_training_time(log_path)\n",
    "                # else:\n",
    "                #     print(f\"Log file not found: {log_path}\")\n",
    "                # print(f\"Total training time: {total_training_time} seconds\")\n",
    "                total += total_training_time\n",
    "        baseline_total_array.append(total)\n",
    "        minutes = int((total % 3600) // 60)\n",
    "        seconds = total % 60\n",
    "        days = int(total // 86400)\n",
    "        hours = int((total % 86400) // 3600)\n",
    "        print(f\"Total training time for {ds}: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time for data_acuteinflammation: 0 days, 0 hours, 20 minutes and 37.20 seconds\n",
      "Total training time for data_balancescale: 0 days, 9 hours, 13 minutes and 25.50 seconds\n",
      "Total training time for data_breastcancerwisc: 0 days, 0 hours, 33 minutes and 20.40 seconds\n",
      "Total training time for data_cardiotocography3clases: 0 days, 8 hours, 40 minutes and 16.20 seconds\n",
      "Total training time for data_energyy1: 0 days, 2 hours, 18 minutes and 30.60 seconds\n",
      "Total training time for data_energyy2: 0 days, 0 hours, 30 minutes and 15.40 seconds\n",
      "Total training time for data_iris: 0 days, 1 hours, 29 minutes and 0.40 seconds\n",
      "Total training time for data_mammographic: 0 days, 0 hours, 29 minutes and 0.40 seconds\n",
      "Total training time for data_pendigits: 0 days, 0 hours, 0 minutes and 0.00 seconds\n",
      "Total training time for data_seeds: 0 days, 2 hours, 59 minutes and 20.40 seconds\n",
      "Total training time for data_tictactoe: 0 days, 3 hours, 8 minutes and 10.50 seconds\n",
      "Total training time for data_vertebralcolumn2clases: 0 days, 7 hours, 7 minutes and 45.40 seconds\n",
      "Total training time for data_vertebralcolumn3clases: 0 days, 0 hours, 39 minutes and 15.40 seconds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def calculate_total_training_time(log_file_path):\n",
    "    \"\"\"\n",
    "    Reads the log file, finds lines with 'Epoch: X' and 'Epoch time: Y',\n",
    "    and computes total training time *assuming* 'Epoch time: Y' is\n",
    "    the average time per epoch for the last (X - previous_epoch) epochs.\n",
    "    \"\"\"\n",
    "    total_time = 0.0\n",
    "    # This regex looks for a line containing something like:\n",
    "    # | Epoch:    50 | Train loss: ... | Epoch time: 0.2 |\n",
    "    pattern = re.compile(r'\\|\\s*Epoch:\\s*(\\d+)\\s*\\|.*Epoch time:\\s+([\\d.]+)')\n",
    "    \n",
    "    prev_epoch = None\n",
    "    with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                current_epoch = int(match.group(1))\n",
    "                epoch_time_avg = float(match.group(2))  # average time per epoch (assumed)\n",
    "                \n",
    "                if prev_epoch is None:\n",
    "                    # For the very first logged epoch, assume that the block covers all epochs from 0 up to current_epoch\n",
    "                    block_size = current_epoch + 1  # e.g., epoch=50 implies epochs 0..50 inclusive => 51 epochs\n",
    "                else:\n",
    "                    # Otherwise, the block covers (prev_epoch+1) .. current_epoch\n",
    "                    block_size = current_epoch - prev_epoch\n",
    "                \n",
    "                # Multiply average time by the block size\n",
    "                total_time += epoch_time_avg * block_size\n",
    "                prev_epoch = current_epoch\n",
    "\n",
    "    return total_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = datasets[0]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    total = 0\n",
    "    proposed_total_array = []\n",
    "    # Replace 'path_to_your_log_file.log' with the actual path to your log file\n",
    "    for i, ds in enumerate(datasets):\n",
    "        total = 0\n",
    "        for seed in range(0,1):\n",
    "            log_path = f'/home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/PowerAwareAugmentedLagrangian/log/{ds}_seed_{seed}_Penalty_AL_Factor_{int(target[0][i]):04d}.log'\n",
    "            total_training_time = 0\n",
    "            if os.path.exists(log_path):\n",
    "                total_training_time = calculate_total_training_time(log_path)\n",
    "            # else:\n",
    "            #     print(f\"Log file not found: {log_path}\")\n",
    "            # print(f\"Total training time: {total_training_time} seconds\")\n",
    "            total += total_training_time\n",
    "        proposed_total_array.append(total)\n",
    "        minutes = int((total % 3600) // 60)\n",
    "        seconds = total % 60\n",
    "        days = int(total // 86400)\n",
    "        hours = int((total % 86400) // 3600)\n",
    "        print(f\"Total training time for {ds}: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498123.09166666795, 11244.816666666666, 97.74256266075867, 44.298018049797875)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-defining the data after environment reset\n",
    "\n",
    "\n",
    "# Remove zeros from both lists\n",
    "filtered_baseline = [b for b in baseline_total_array if b > 0]\n",
    "filtered_proposed = [p for p in proposed_total_array if p > 0]\n",
    "\n",
    "# Calculate average times\n",
    "avg_baseline = sum(filtered_baseline) / len(filtered_baseline)\n",
    "avg_proposed = sum(filtered_proposed) / len(filtered_proposed)\n",
    "\n",
    "# Calculate reduction percentages\n",
    "time_reduction = (1 - (avg_proposed / avg_baseline)) * 100\n",
    "relative_time_ratio = avg_baseline / avg_proposed\n",
    "\n",
    "avg_baseline, avg_proposed, time_reduction, relative_time_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8302.0515277778, 187.4136111111111)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "498123.09166666795/60, 11244.816666666666/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189.67791666666665"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.264305555555555 + 187.4136111111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309.60291666666666, 18.152361111110785)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18576.175/60, 1089.141666666647/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time for data_acuteinflammation: 0 days, 0 hours, 3 minutes and 10.30 seconds\n",
      "Total training time for data_balancescale: 0 days, 0 hours, 0 minutes and 30.30 seconds\n",
      "Total training time for data_breastcancerwisc: 0 days, 0 hours, 3 minutes and 10.40 seconds\n",
      "Total training time for data_cardiotocography3clases: 0 days, 0 hours, 0 minutes and 46.10 seconds\n",
      "Total training time for data_energyy1: 0 days, 0 hours, 3 minutes and 10.40 seconds\n",
      "Total training time for data_energyy2: 0 days, 0 hours, 3 minutes and 15.30 seconds\n",
      "Total training time for data_iris: 0 days, 0 hours, 2 minutes and 30.40 seconds\n",
      "Total training time for data_mammographic: 0 days, 0 hours, 3 minutes and 15.40 seconds\n",
      "Log file not found: /home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/v3_MlflowPowerAwareAugmentedLagrangian/log/data_pendigits_seed_0_Penalty_AL_Factor_0000.log\n",
      "Total training time for data_pendigits: 0 days, 0 hours, 0 minutes and 0.00 seconds\n",
      "Total training time for data_seeds: 0 days, 0 hours, 3 minutes and 10.40 seconds\n",
      "Total training time for data_tictactoe: 0 days, 0 hours, 3 minutes and 10.40 seconds\n",
      "Total training time for data_vertebralcolumn2clases: 0 days, 0 hours, 0 minutes and 50.50 seconds\n",
      "Total training time for data_vertebralcolumn3clases: 0 days, 0 hours, 0 minutes and 10.40 seconds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def calculate_total_training_time(log_file_path):\n",
    "    \"\"\"\n",
    "    Reads the log file, finds lines with 'Epoch: X' and 'Epoch time: Y',\n",
    "    and computes total training time *assuming* 'Epoch time: Y' is\n",
    "    the average time per epoch for the last (X - previous_epoch) epochs.\n",
    "    \"\"\"\n",
    "    total_time = 0.0\n",
    "    # This regex looks for a line containing something like:\n",
    "    # | Epoch:    50 | Train loss: ... | Epoch time: 0.2 |\n",
    "    pattern = re.compile(r'\\|\\s*Epoch:\\s*(\\d+)\\s*\\|.*Epoch time:\\s+([\\d.]+)')\n",
    "    \n",
    "    prev_epoch = None\n",
    "    with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                current_epoch = int(match.group(1))\n",
    "                epoch_time_avg = float(match.group(2))  # average time per epoch (assumed)\n",
    "                \n",
    "                if prev_epoch is None:\n",
    "                    # For the very first logged epoch, assume that the block covers all epochs from 0 up to current_epoch\n",
    "                    block_size = current_epoch + 1  # e.g., epoch=50 implies epochs 0..50 inclusive => 51 epochs\n",
    "                else:\n",
    "                    # Otherwise, the block covers (prev_epoch+1) .. current_epoch\n",
    "                    block_size = current_epoch - prev_epoch\n",
    "                \n",
    "                # Multiply average time by the block size\n",
    "                total_time += epoch_time_avg * block_size\n",
    "                prev_epoch = current_epoch\n",
    "\n",
    "    return total_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = datasets[0]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    # Replace 'path_to_your_log_file.log' with the actual path to your log file\n",
    "    mlflow_total_array = []\n",
    "    for ds in datasets:\n",
    "        total = 0\n",
    "        log_path = f'/home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/v3_MlflowPowerAwareAugmentedLagrangian/log/{ds}_seed_{0}_Penalty_AL_Factor_0000.log'\n",
    "        total_training_time = 0\n",
    "        if os.path.exists(log_path):\n",
    "            total_training_time = calculate_total_training_time(log_path)\n",
    "        else:\n",
    "            print(f\"Log file not found: {log_path}\")\n",
    "        # print(f\"Total training time: {total_training_time} seconds\")\n",
    "        total += total_training_time\n",
    "        mlflow_total_array.append(total)\n",
    "        minutes = int((total % 3600) // 60)\n",
    "        seconds = total % 60\n",
    "        days = int(total // 86400)\n",
    "        hours = int((total % 86400) // 3600)\n",
    "        print(f\"Total training time for {ds}: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.85833333333332"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-defining the data after environment reset\n",
    "\n",
    "\n",
    "# Remove zeros from both lists\n",
    "filtered_mlflow = [b for b in mlflow_total_array if b > 0]\n",
    "\n",
    "# Calculate average times\n",
    "avg_mlflow = sum(filtered_mlflow) / len(filtered_mlflow)\n",
    "\n",
    "avg_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.264305555555555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "135.85833333333332/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 0 days, 0 hours, 18 minutes and 9.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_total = avg_proposed\n",
    "minutes = int((mean_total % 3600) // 60)\n",
    "seconds = mean_total % 60\n",
    "days = int(mean_total // 86400)\n",
    "hours = int((mean_total % 86400) // 3600)\n",
    "print(f\"Total training time: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([27681.300000000003,\n",
       "  11748.9,\n",
       "  15644.69999999998,\n",
       "  42854.00000000001,\n",
       "  22705.099999999988,\n",
       "  23772.300000000036,\n",
       "  13790.399999999996,\n",
       "  13965.899999999996,\n",
       "  0,\n",
       "  12467.299999999994,\n",
       "  15468.499999999993,\n",
       "  11545.899999999992,\n",
       "  11269.8],\n",
       " [148.09999999999826,\n",
       "  1870.4999999998458,\n",
       "  85.09999999999948,\n",
       "  2162.6000000000236,\n",
       "  423.4000000000096,\n",
       "  133.89999999999986,\n",
       "  442.19999999999794,\n",
       "  1395.599999999886,\n",
       "  0,\n",
       "  2193.3000000001725,\n",
       "  1140.9999999999288,\n",
       "  1828.6999999999882,\n",
       "  1245.2999999999147])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_total_array, proposed_total_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([27681.300000000003,\n",
       "  11748.9,\n",
       "  15644.69999999998,\n",
       "  42854.00000000001,\n",
       "  22705.099999999988,\n",
       "  23772.300000000036,\n",
       "  13790.399999999996,\n",
       "  13965.899999999996,\n",
       "  0,\n",
       "  12467.299999999994,\n",
       "  15468.499999999993,\n",
       "  11545.899999999992,\n",
       "  11269.8],\n",
       " [148.09999999999826,\n",
       "  1870.4999999998458,\n",
       "  85.09999999999948,\n",
       "  2162.6000000000236,\n",
       "  423.4000000000096,\n",
       "  133.89999999999986,\n",
       "  442.19999999999794,\n",
       "  1395.599999999886,\n",
       "  0,\n",
       "  2193.3000000001725,\n",
       "  1140.9999999999288,\n",
       "  1828.6999999999882,\n",
       "  1245.2999999999147])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_total_array, proposed_total_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# log_file_path = '/home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/PowerAwareAugmentedLagrangianHS/log/data_acuteinflammation_seed_0_Penalty_AL_Factor_0200.log'\n",
    "baseline_total_array = []\n",
    "for ds in datasets:\n",
    "    total = 0\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,100, 2):\n",
    "            log_file_path = f'/home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/PowerAwareBaselineHS/log/{ds}_seed_{i}_Penalty_power_Factor_{j}.log'\n",
    "            # Read the log file\n",
    "            if j == 58:\n",
    "                continue\n",
    "            try:\n",
    "                with open(log_file_path, 'r') as file:\n",
    "                    log_lines = file.readlines()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Log file not found: {log_file_path}\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # # Extract timestamps\n",
    "            # timestamps = []\n",
    "            # for line in log_lines:\n",
    "            #     match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}', line)\n",
    "            #     if match:\n",
    "            #         timestamps.append(datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S,%f'))\n",
    "\n",
    "            # # Calculate total training time considering multiple sessions\n",
    "            # Calculate total training time considering multiple sessions\n",
    "            total_training_time = 0\n",
    "            start_time = None\n",
    "\n",
    "            for line in log_lines:\n",
    "                if \"Training network on device: cpu.\" in line:\n",
    "                    start_time = None  # Reset the start time\n",
    "                if \"Training was already finished.\" in line:\n",
    "                    break\n",
    "                else:\n",
    "                    match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}', line)\n",
    "                    if match:\n",
    "                        current_time = datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S,%f')\n",
    "                        if start_time is not None:\n",
    "                            delta = (current_time - start_time).total_seconds()\n",
    "                            total_training_time += delta\n",
    "                        start_time = current_time\n",
    "            # if timestamps:\n",
    "            #     total_training_time = sum((timestamps[i] - timestamps[i - 1]).total_seconds() for i in range(1, len(timestamps)))\n",
    "            #     print(f\"Total training time: {total_training_time} seconds\")\n",
    "            # else:\n",
    "            #     print(\"No timestamps found in the log file.\")\n",
    "            \n",
    "            total += total_training_time\n",
    "    minutes = int((total % 3600) // 60)\n",
    "    seconds = total % 60\n",
    "    days = int(total // 86400)\n",
    "    hours = int((total % 86400) // 3600)\n",
    "    print(f\"Dataset: {ds}\")\n",
    "    print(f\"Total training time: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")\n",
    "    baseline_total_array.append(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[301452833.7129997,\n",
       " 305486729.6469996,\n",
       " 304775707.1890001,\n",
       " 297958326.01500016,\n",
       " 302933716.55099976,\n",
       " 290528358.09800017,\n",
       " 304824297.86800003,\n",
       " 305147568.0889999,\n",
       " 0,\n",
       " 305334140.53900003,\n",
       " 304810356.84200007,\n",
       " 305606236.00200003,\n",
       " 305684637.69299984]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_total_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297958326.01500016"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "total = np.mean(baseline_total_array)\n",
    "total = baseline_total_array[3]\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[989629.0160000001,\n",
       " 747224.6640000003,\n",
       " 842461.8100000002,\n",
       " 2910353.376000002,\n",
       " 302405801.02400017,\n",
       " 302047625.7630001,\n",
       " 304408486.0039997,\n",
       " 303469710.964,\n",
       " 0,\n",
       " 304245809.47299993,\n",
       " 303908692.1930001,\n",
       " 304897891.93299997,\n",
       " 303762392.625]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_total_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 82766 hours 12 minutes 6.02 seconds\n",
      "Total training time: 3448 days, 14 hours, 12 minutes and 6.02 seconds\n"
     ]
    }
   ],
   "source": [
    "minutes = int((total % 3600) // 60)\n",
    "seconds = total % 60\n",
    "days = int(total // 86400)\n",
    "hours = int((total % 86400) // 3600)\n",
    "print(f\"Total training time: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 421906.49700000003 seconds\n",
      "Total training time: 752232.4269999999 seconds\n",
      "Total training time: 752193.716 seconds\n",
      "Total training time: 747732.25 seconds\n",
      "Total training time: 702051.645 seconds\n",
      "Total training time: 1031941.4550000001 seconds\n",
      "Total training time: 1032028.729 seconds\n",
      "Total training time: 1031941.9330000001 seconds\n",
      "Log file not found: /home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/PowerAwareAugmentedLagrangianHS/log/data_pendigits_seed_0_Penalty_AL_Factor_0600.log\n",
      "Total training time: 1031907.553 seconds\n",
      "Total training time: 1031826.9 seconds\n",
      "Total training time: 1031823.306 seconds\n",
      "Total training time: 1031789.056 seconds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "proposed_total_array = []\n",
    "for i, ds in enumerate(datasets):\n",
    "    log_file_path = f'/home/kit/itec/qc0876/projects/PowerAwarePNN/PowerAwarePNN/maincode/PowerAwareAugmentedLagrangianHS/log/{ds}_seed_0_Penalty_AL_Factor_{int(target[0][i]):04d}.log'\n",
    "    # Read the log file\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            log_lines = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Log file not found: {log_file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract timestamps\n",
    "    timestamps = []\n",
    "    for line in log_lines:\n",
    "        match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}', line)\n",
    "        if match:\n",
    "            timestamps.append(datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S,%f'))\n",
    "\n",
    "    # Calculate total training time considering multiple sessions\n",
    "    if timestamps:\n",
    "        total_training_time = sum((timestamps[i] - timestamps[i - 1]).total_seconds() for i in range(1, len(timestamps)))\n",
    "        print(f\"Total training time: {total_training_time} seconds\")\n",
    "    else:\n",
    "        print(\"No timestamps found in the log file.\")\n",
    "    proposed_total_array.append(total_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 117 hours 11 minutes 46.50 seconds\n",
      "Total training time: 4 days, 21 hours, 11 minutes and 46.50 seconds\n"
     ]
    }
   ],
   "source": [
    "hours = int(total_training_time // 3600)\n",
    "minutes = int((total_training_time % 3600) // 60)\n",
    "seconds = total_training_time % 60\n",
    "print(f\"Total training time: {hours} hours {minutes} minutes {seconds:.2f} seconds\")\n",
    "days = int(total_training_time // 86400)\n",
    "hours = int((total_training_time % 86400) // 3600)\n",
    "print(f\"Total training time: {days} days, {hours} hours, {minutes} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'configuration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(Path(os\u001b[38;5;241m.\u001b[39mgetcwd())\u001b[38;5;241m.\u001b[39mparent))\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfiguration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'configuration'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
    "from configuration import *\n",
    "import torch\n",
    "import pprint\n",
    "import pNN_Power_Aware as pNN\n",
    "from utils import *\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args = FormulateArgs(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pNN_Power_Aware'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./maincode/PowerAwareBaselineHS/models/pNN_data_tictactoe_seed_4_Penalty_power_Factor_68_FT.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model_PT \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/train-model/lib/python3.12/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/train-model/lib/python3.12/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/train-model/lib/python3.12/site-packages/torch/serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pNN_Power_Aware'"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"./maincode/PowerAwareBaselineHS/models/pNN_data_tictactoe_seed_4_Penalty_power_Factor_68_FT.model\"\n",
    "\n",
    "model_PT = torch.load(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
